{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ed58d6f-70c2-4dcb-ba60-c503e1803fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   duration  protocol_type  service  flag  src_bytes  dst_bytes  land  \\\n",
      "0         0              1       45     1          0          0     0   \n",
      "1         0              1       45     1          0          0     0   \n",
      "2         2              1       19     9      12983          0     0   \n",
      "3         0              0       13     9         20          0     0   \n",
      "4         1              1       55     2          0         15     0   \n",
      "\n",
      "   wrong_fragment  urgent  hot  ...  dst_host_srv_count  \\\n",
      "0               0       0    0  ...                  10   \n",
      "1               0       0    0  ...                   1   \n",
      "2               0       0    0  ...                  86   \n",
      "3               0       0    0  ...                  57   \n",
      "4               0       0    0  ...                  86   \n",
      "\n",
      "   dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
      "0                    0.04                    0.06   \n",
      "1                    0.00                    0.06   \n",
      "2                    0.61                    0.04   \n",
      "3                    1.00                    0.00   \n",
      "4                    0.31                    0.17   \n",
      "\n",
      "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
      "0                         0.00                         0.00   \n",
      "1                         0.00                         0.00   \n",
      "2                         0.61                         0.02   \n",
      "3                         1.00                         0.28   \n",
      "4                         0.03                         0.02   \n",
      "\n",
      "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
      "0                   0.0                       0.0                  1.00   \n",
      "1                   0.0                       0.0                  1.00   \n",
      "2                   0.0                       0.0                  0.00   \n",
      "3                   0.0                       0.0                  0.00   \n",
      "4                   0.0                       0.0                  0.83   \n",
      "\n",
      "   dst_host_srv_rerror_rate    class  \n",
      "0                      1.00  anomaly  \n",
      "1                      1.00  anomaly  \n",
      "2                      0.00   normal  \n",
      "3                      0.00  anomaly  \n",
      "4                      0.71  anomaly  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Step 2: Load the dataset  # Replace with your CSV file path\n",
    "data = pd.read_csv(r\"C:\\Users\\shubh\\OneDrive\\Desktop\\SOC_cyberguard\\Train_data.csv\")\n",
    "data2 = pd.read_csv(r\"C:\\Users\\shubh\\OneDrive\\Desktop\\SOC_cyberguard\\Test_data.csv\")\n",
    "\n",
    "# Step 3: Preprocess the data\n",
    "# Drop non-numeric columns except 'class'\n",
    "data_numeric = data.drop(columns=['protocol_type', 'service', 'flag'])\n",
    "data_numeric_2 = data2.drop(columns=['protocol_type', 'service', 'flag'])\n",
    "\n",
    "# Encode categorical columns if necessary, basically putting a number tag on text data in the cells\n",
    "label_encoders = {}\n",
    "for column in ['protocol_type', 'service', 'flag']:\n",
    "    le = LabelEncoder()\n",
    "    data[column] = le.fit_transform(data[column])\n",
    "    data2[column] = le.fit_transform(data2[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Concatenating the removed columns (ones who had text) and replacing them with numbers\n",
    "data_preprocessed = pd.concat([data_numeric.drop(columns=['class']), data[['protocol_type', 'service', 'flag']]], axis=1)\n",
    "data_preprocessed_2 = pd.concat([data_numeric_2, data2[['protocol_type', 'service', 'flag']]], axis=1)\n",
    "\n",
    "# Extract the target variable\n",
    "y = data['class'].map({'normal': 0, 'anomaly': 1})\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data_preprocessed)\n",
    "scaled_data2 = scaler.transform(data_preprocessed_2)  # Standardize the test data using the same scaler\n",
    "\n",
    "# Step 4: Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=10)\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "principal_components2 = pca.transform(scaled_data2)  # Apply PCA to the test data using the same PCA model\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train = principal_components\n",
    "y_train = y\n",
    "X_test = principal_components2\n",
    "\n",
    "# Step 5: Train logistic regression model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Map numerical predictions back to class labels\n",
    "y_pred_labels = pd.Series(y_pred).map({0: 'normal', 1: 'anomaly'})\n",
    "\n",
    "# Add the predicted class labels to data2\n",
    "data2['class'] = y_pred_labels\n",
    "\n",
    "# Save the scaler, PCA, and logistic regression model to disk\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(pca, 'pca.pkl')\n",
    "joblib.dump(log_reg, 'log_reg.pkl')\n",
    "joblib.dump(label_encoders, 'label_encoders.pkl')\n",
    "\n",
    "# Print out the first few rows of the updated data2 to see the results\n",
    "print(data2.head())\n",
    "\n",
    "# Save the updated data2 as a new CSV file\n",
    "data2.to_csv(r\"C:\\Users\\shubh\\OneDrive\\Desktop\\SOC_cyberguard\\logreg_test_data_result.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffc13b90-b6f6-43da-b5d7-b9d75ee037e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   duration  protocol_type  service  flag  src_bytes  dst_bytes  land  \\\n",
      "0         0              1       45     1          0          0     0   \n",
      "1         0              1       45     1          0          0     0   \n",
      "2         2              1       19     9      12983          0     0   \n",
      "3         0              0       13     9         20          0     0   \n",
      "4         1              1       55     2          0         15     0   \n",
      "\n",
      "   wrong_fragment  urgent  hot  ...  dst_host_srv_count  \\\n",
      "0               0       0    0  ...                  10   \n",
      "1               0       0    0  ...                   1   \n",
      "2               0       0    0  ...                  86   \n",
      "3               0       0    0  ...                  57   \n",
      "4               0       0    0  ...                  86   \n",
      "\n",
      "   dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
      "0                    0.04                    0.06   \n",
      "1                    0.00                    0.06   \n",
      "2                    0.61                    0.04   \n",
      "3                    1.00                    0.00   \n",
      "4                    0.31                    0.17   \n",
      "\n",
      "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
      "0                         0.00                         0.00   \n",
      "1                         0.00                         0.00   \n",
      "2                         0.61                         0.02   \n",
      "3                         1.00                         0.28   \n",
      "4                         0.03                         0.02   \n",
      "\n",
      "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
      "0                   0.0                       0.0                  1.00   \n",
      "1                   0.0                       0.0                  1.00   \n",
      "2                   0.0                       0.0                  0.00   \n",
      "3                   0.0                       0.0                  0.00   \n",
      "4                   0.0                       0.0                  0.83   \n",
      "\n",
      "   dst_host_srv_rerror_rate    class  \n",
      "0                      1.00  anomaly  \n",
      "1                      1.00  anomaly  \n",
      "2                      0.00   normal  \n",
      "3                      0.00  anomaly  \n",
      "4                      0.71   normal  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Step 2: Load the dataset  # Replace with your CSV file path\n",
    "data = pd.read_csv(r\"C:\\Users\\shubh\\OneDrive\\Desktop\\SOC_cyberguard\\Train_data.csv\")\n",
    "data2 = pd.read_csv(r\"C:\\Users\\shubh\\OneDrive\\Desktop\\SOC_cyberguard\\Test_data.csv\")\n",
    "\n",
    "# Step 3: Preprocess the data\n",
    "# Drop non-numeric columns except 'class'\n",
    "data_numeric = data.drop(columns=['protocol_type', 'service', 'flag'])\n",
    "data_numeric_2 = data2.drop(columns=['protocol_type', 'service', 'flag'])\n",
    "\n",
    "# Encode categorical columns if necessary, basically putting a number tag on text data in the cells\n",
    "label_encoders = {}\n",
    "for column in ['protocol_type', 'service', 'flag']:\n",
    "    le = LabelEncoder()\n",
    "    data[column] = le.fit_transform(data[column])\n",
    "    data2[column] = le.fit_transform(data2[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Concatenate the removed columns (ones who had text) and replace them with numbers\n",
    "data_preprocessed = pd.concat([data_numeric.drop(columns=['class']), data[['protocol_type', 'service', 'flag']]], axis=1)\n",
    "data_preprocessed_2 = pd.concat([data_numeric_2, data2[['protocol_type', 'service', 'flag']]], axis=1)\n",
    "\n",
    "# Extract the target variable\n",
    "y = data['class'].map({'normal': 0, 'anomaly': 1})\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data_preprocessed)\n",
    "scaled_data2 = scaler.transform(data_preprocessed_2)\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "principal_components2 = pca.transform(scaled_data2) \n",
    "\n",
    "X_train = principal_components\n",
    "y_train = y\n",
    "X_test = principal_components2\n",
    "\n",
    "# Step 5: Train Decision Tree Classifier model\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "# Map numerical predictions back to class labels\n",
    "y_pred_labels = pd.Series(y_pred).map({0: 'normal', 1: 'anomaly'})\n",
    "\n",
    "# Add the predicted class labels to data2\n",
    "data2['class'] = y_pred_labels\n",
    "\n",
    "# Save the scaler, PCA, and logistic regression model to disk\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(pca, 'pca.pkl')\n",
    "joblib.dump(log_reg, 'log_reg.pkl')\n",
    "joblib.dump(label_encoders, 'label_encoders.pkl')\n",
    "\n",
    "# Print out the first few rows of the updated data2 to see the results\n",
    "print(data2.head())\n",
    "\n",
    "# Save the updated data2 as a new CSV file\n",
    "data2.to_csv(r\"C:\\Users\\shubh\\OneDrive\\Desktop\\SOC_cyberguard\\DecTree_test_data_result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9317563d-0bb8-4df8-a76b-6e1d0a56dd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   duration  protocol_type  service  flag  src_bytes  dst_bytes  land  \\\n",
      "0         0              1       45     1          0          0     0   \n",
      "1         0              1       45     1          0          0     0   \n",
      "2         2              1       19     9      12983          0     0   \n",
      "3         0              0       13     9         20          0     0   \n",
      "4         1              1       55     2          0         15     0   \n",
      "\n",
      "   wrong_fragment  urgent  hot  ...  dst_host_srv_count  \\\n",
      "0               0       0    0  ...                  10   \n",
      "1               0       0    0  ...                   1   \n",
      "2               0       0    0  ...                  86   \n",
      "3               0       0    0  ...                  57   \n",
      "4               0       0    0  ...                  86   \n",
      "\n",
      "   dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
      "0                    0.04                    0.06   \n",
      "1                    0.00                    0.06   \n",
      "2                    0.61                    0.04   \n",
      "3                    1.00                    0.00   \n",
      "4                    0.31                    0.17   \n",
      "\n",
      "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
      "0                         0.00                         0.00   \n",
      "1                         0.00                         0.00   \n",
      "2                         0.61                         0.02   \n",
      "3                         1.00                         0.28   \n",
      "4                         0.03                         0.02   \n",
      "\n",
      "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
      "0                   0.0                       0.0                  1.00   \n",
      "1                   0.0                       0.0                  1.00   \n",
      "2                   0.0                       0.0                  0.00   \n",
      "3                   0.0                       0.0                  0.00   \n",
      "4                   0.0                       0.0                  0.83   \n",
      "\n",
      "   dst_host_srv_rerror_rate    class  \n",
      "0                      1.00  anomaly  \n",
      "1                      1.00  anomaly  \n",
      "2                      0.00   normal  \n",
      "3                      0.00  anomaly  \n",
      "4                      0.71  anomaly  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Step 2: Load the dataset  # Replace with your CSV file path\n",
    "data = pd.read_csv(r\"C:\\Users\\shubh\\OneDrive\\Desktop\\SOC_cyberguard\\Train_data.csv\")\n",
    "data2 = pd.read_csv(r\"C:\\Users\\shubh\\OneDrive\\Desktop\\SOC_cyberguard\\Test_data.csv\")\n",
    "\n",
    "# Step 3: Preprocess the data\n",
    "# Drop non-numeric columns except 'class'\n",
    "data_numeric = data.drop(columns=['protocol_type', 'service', 'flag'])\n",
    "data_numeric_2 = data2.drop(columns=['protocol_type', 'service', 'flag'])\n",
    "\n",
    "# Encode categorical columns if necessary, basically putting a number tag on text data in the cells\n",
    "label_encoders = {}\n",
    "for column in ['protocol_type', 'service', 'flag']:\n",
    "    le = LabelEncoder()\n",
    "    data[column] = le.fit_transform(data[column])\n",
    "    data2[column] = le.fit_transform(data2[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Concatenate the removed columns (ones who had text) and replace them with numbers\n",
    "data_preprocessed = pd.concat([data_numeric.drop(columns=['class']), data[['protocol_type', 'service', 'flag']]], axis=1)\n",
    "data_preprocessed_2 = pd.concat([data_numeric_2, data2[['protocol_type', 'service', 'flag']]], axis=1)\n",
    "\n",
    "# Extract the target variable\n",
    "y = data['class'].map({'normal': 0, 'anomaly': 1})\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data_preprocessed)\n",
    "scaled_data2 = scaler.transform(data_preprocessed_2)\n",
    "\n",
    "# Step 4: Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=10)\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "principal_components2 = pca.transform(scaled_data2)\n",
    "\n",
    "X_train = principal_components\n",
    "y_train = y\n",
    "X_test = principal_components2\n",
    "\n",
    "# Step 5: Train K-Nearest Neighbors model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "y_pred_labels = pd.Series(y_pred).map({0: 'normal', 1: 'anomaly'})\n",
    "\n",
    "# Add the predicted class labels to data2\n",
    "data2['class'] = y_pred_labels\n",
    "\n",
    "# Save the scaler, PCA, and logistic regression model to disk\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(pca, 'pca.pkl')\n",
    "joblib.dump(log_reg, 'log_reg.pkl')\n",
    "joblib.dump(label_encoders, 'label_encoders.pkl')\n",
    "\n",
    "# Print out the first few rows of the updated data2 to see the results\n",
    "print(data2.head())\n",
    "\n",
    "# Save the updated data2 as a new CSV file\n",
    "data2.to_csv(r\"C:\\Users\\shubh\\OneDrive\\Desktop\\SOC_cyberguard\\KNeighbours_test_data_result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846d488e-a53b-44d7-88bf-88aed10e7fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
