{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6949be6-8e6e-4bc9-8885-047a3c3236de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data set before feature scaling \n",
      "\n",
      "[[0 'tcp' 'ftp_data' ... 0.0 0.05 0.0]\n",
      " [0 'udp' 'other' ... 0.0 0.0 0.0]\n",
      " [0 'tcp' 'private' ... 1.0 0.0 0.0]\n",
      " ...\n",
      " [0 'tcp' 'private' ... 0.0 1.0 1.0]\n",
      " [0 'tcp' 'nnsp' ... 1.0 0.0 0.0]\n",
      " [0 'tcp' 'finger' ... 1.0 0.0 0.0]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'tcp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 67\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m     66\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m---> 67\u001b[0m X \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata set after feature scaling \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:876\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:912\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \n\u001b[0;32m    882\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    911\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 912\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    913\u001b[0m     X,\n\u001b[0;32m    914\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    915\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[0;32m    916\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    917\u001b[0m     reset\u001b[38;5;241m=\u001b[39mfirst_call,\n\u001b[0;32m    918\u001b[0m )\n\u001b[0;32m    919\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:997\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    995\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 997\u001b[0m         array \u001b[38;5;241m=\u001b[39m _asarray_with_order(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1001\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:521\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    519\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 521\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'tcp'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, epochs=3000, regularization_strength=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.regularization_strength = regularization_strength\n",
    "        self.cost_history = []\n",
    "\n",
    "    #sigmoid function to squish everything between 0 to 1, the premise of logistic regression\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    #cost function \n",
    "    def compute_cost(self, y, y_predicted):\n",
    "        n_samples = len(y)\n",
    "        cost = (-1 / n_samples) * (np.dot(y, np.log(y_predicted)) + np.dot((1 - y), np.log(1 - y_predicted)))\n",
    "        cost += (self.regularization_strength / (2 * n_samples)) * np.sum(np.square(self.weights))\n",
    "        return cost\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)  # Initializing all parameters with zero\n",
    "        self.bias = 0\n",
    "\n",
    "        # Gradient descent\n",
    "        for _ in range(self.epochs):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self.sigmoid(linear_model)\n",
    "\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y)) + (self.regularization_strength * self.weights)\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "            # Compute and store cost\n",
    "            cost = self.compute_cost(y, y_predicted)\n",
    "            self.cost_history.append(cost)\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = self.sigmoid(linear_model)\n",
    "        class_pred = [0 if y <= 0.5 else 1 for y in y_pred]\n",
    "        return class_pred\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(r\"C:\\Users\\shubh\\OneDrive\\Desktop\\SOC_cyberguard\\Train_data.csv\")\n",
    "    \n",
    "    # first 13 columns are features, 14th column is target, so in 'y'\n",
    "   \n",
    "    X = df.iloc[:, :-1].values\n",
    "    y = df.iloc[:, -1].values\n",
    "\n",
    "    print(\"data set before feature scaling \\n\")\n",
    "    print(X)\n",
    "    print('\\n')\n",
    "    # Scaling done, basically each of the mentioned feature is squeezed down according to the formula z = (x - mean)/variance , where variance is 1\n",
    "    #this is done in logistic regression to improve the model's accuracy and prevent any one feature to dominate the model, and creates a level field\n",
    "   \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    print(\"data set after feature scaling \\n\")\n",
    "    print(X)\n",
    "    print('\\n')\n",
    "    \n",
    "    # Train-test split using the train_test_split function, which allots, 20% of the input data to be used to test the model and 80% to train \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Trainin the model using above class\n",
    "    model = LogisticRegression(learning_rate=0.01, epochs=3000, regularization_strength=0.01)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Checking model accuracy\n",
    "    def accuracy(predictions, y_test):\n",
    "        return np.sum(predictions == y_test) / len(y_test)\n",
    "\n",
    "    acc = accuracy(predictions, y_test)\n",
    "    print(f\"Accuracy: {acc * 100}%\")\n",
    "\n",
    "    # Plotting the change of cost function over the iterations/epochs\n",
    "    plt.plot(range(model.epochs), model.cost_history)\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('Cost function')\n",
    "    plt.title('Cost Function during Training')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42c81004-81bc-4f94-90bb-ee49c51449fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   duration  protocol_type  service  flag  src_bytes  dst_bytes  land  \\\n",
      "0         0              1       19     9        491          0     0   \n",
      "1         0              2       41     9        146          0     0   \n",
      "2         0              1       46     5          0          0     0   \n",
      "3         0              1       22     9        232       8153     0   \n",
      "4         0              1       22     9        199        420     0   \n",
      "\n",
      "   wrong_fragment  urgent  hot  ...  dst_host_srv_count  \\\n",
      "0               0       0    0  ...                  25   \n",
      "1               0       0    0  ...                   1   \n",
      "2               0       0    0  ...                  26   \n",
      "3               0       0    0  ...                 255   \n",
      "4               0       0    0  ...                 255   \n",
      "\n",
      "   dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
      "0                    0.17                    0.03   \n",
      "1                    0.00                    0.60   \n",
      "2                    0.10                    0.05   \n",
      "3                    1.00                    0.00   \n",
      "4                    1.00                    0.00   \n",
      "\n",
      "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
      "0                         0.17                         0.00   \n",
      "1                         0.88                         0.00   \n",
      "2                         0.00                         0.00   \n",
      "3                         0.03                         0.04   \n",
      "4                         0.00                         0.00   \n",
      "\n",
      "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
      "0                  0.00                      0.00                  0.05   \n",
      "1                  0.00                      0.00                  0.00   \n",
      "2                  1.00                      1.00                  0.00   \n",
      "3                  0.03                      0.01                  0.00   \n",
      "4                  0.00                      0.00                  0.00   \n",
      "\n",
      "   dst_host_srv_rerror_rate    class  \n",
      "0                      0.00   normal  \n",
      "1                      0.00   normal  \n",
      "2                      0.00  anomaly  \n",
      "3                      0.01   normal  \n",
      "4                      0.00   normal  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Install scikit-learn if not already installed\n",
    "try:\n",
    "    import sklearn\n",
    "except ModuleNotFoundError:\n",
    "    import os\n",
    "    os.system('pip install scikit-learn')\n",
    "    import sklearn\n",
    "    import joblib\n",
    "\n",
    "# Step 1: Import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "# Step 2: Load the dataset  # Replace with your CSV file path\n",
    "data = pd.read_csv(r\"C:\\Users\\shubh\\OneDrive\\Desktop\\SOC_cyberguard\\Train_data.csv\")\n",
    "\n",
    "# Step 3: Preprocess the data\n",
    "# Drop non-numeric columns except 'class'\n",
    "data_numeric = data.drop(columns=['protocol_type', 'service', 'flag'])\n",
    "\n",
    "# Encode categorical columns if necessary\n",
    "label_encoders = {}\n",
    "for column in ['protocol_type', 'service', 'flag']:\n",
    "    le = LabelEncoder()\n",
    "    data[column] = le.fit_transform(data[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Combine numeric and encoded categorical columns\n",
    "data_preprocessed = pd.concat([data_numeric.drop(columns=['class']), data[['protocol_type', 'service', 'flag']]], axis=1)\n",
    "\n",
    "# Extract the target variable\n",
    "y = data['class'].map({'normal': 0, 'anomaly': 1})\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data_preprocessed)\n",
    "\n",
    "# Step 4: Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=10)\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Step 5: Train logistic regression model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(principal_components, y)\n",
    "\n",
    "# Save the scaler, PCA, and logistic regression model to disk\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(pca, 'pca.pkl')\n",
    "joblib.dump(log_reg, 'log_reg.pkl')\n",
    "joblib.dump(label_encoders, 'label_encoders.pkl')\n",
    "\n",
    "# Print out the first few rows to see the results\n",
    "print(data.head())\n",
    "\n",
    "# Save the preprocessed data as a new CSV file\n",
    "data.to_csv(r\"C:\\Users\\shubh\\OneDrive\\Desktop\\SOC_cyberguard\\test_data_results.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ca532aa6-f8ee-49b3-bcdc-ff2711638eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 94.86\n",
      "Classification Report for Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95      2756\n",
      "           1       0.96      0.93      0.94      2436\n",
      "\n",
      "    accuracy                           0.95      5192\n",
      "   macro avg       0.95      0.95      0.95      5192\n",
      "weighted avg       0.95      0.95      0.95      5192\n",
      "\n",
      "   duration  protocol_type  service  flag  src_bytes  dst_bytes  land  \\\n",
      "0         0              1       19     9        491          0     0   \n",
      "1         0              2       41     9        146          0     0   \n",
      "2         0              1       46     5          0          0     0   \n",
      "3         0              1       22     9        232       8153     0   \n",
      "4         0              1       22     9        199        420     0   \n",
      "\n",
      "   wrong_fragment  urgent  hot  ...  dst_host_srv_count  \\\n",
      "0               0       0    0  ...                  25   \n",
      "1               0       0    0  ...                   1   \n",
      "2               0       0    0  ...                  26   \n",
      "3               0       0    0  ...                 255   \n",
      "4               0       0    0  ...                 255   \n",
      "\n",
      "   dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
      "0                    0.17                    0.03   \n",
      "1                    0.00                    0.60   \n",
      "2                    0.10                    0.05   \n",
      "3                    1.00                    0.00   \n",
      "4                    1.00                    0.00   \n",
      "\n",
      "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
      "0                         0.17                         0.00   \n",
      "1                         0.88                         0.00   \n",
      "2                         0.00                         0.00   \n",
      "3                         0.03                         0.04   \n",
      "4                         0.00                         0.00   \n",
      "\n",
      "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
      "0                  0.00                      0.00                  0.05   \n",
      "1                  0.00                      0.00                  0.00   \n",
      "2                  1.00                      1.00                  0.00   \n",
      "3                  0.03                      0.01                  0.00   \n",
      "4                  0.00                      0.00                  0.00   \n",
      "\n",
      "   dst_host_srv_rerror_rate    class  \n",
      "0                      0.00   normal  \n",
      "1                      0.00   normal  \n",
      "2                      0.00  anomaly  \n",
      "3                      0.01   normal  \n",
      "4                      0.00   normal  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Step 2: Load the dataset  # Replace with your CSV file path\n",
    "data = pd.read_csv(r\"C:\\Users\\shubh\\OneDrive\\Desktop\\SOC_cyberguard\\Train_data.csv\")\n",
    "\n",
    "# Step 3: Preprocess the data\n",
    "# Drop non-numeric columns except 'class'\n",
    "data_numeric = data.drop(columns=['protocol_type', 'service', 'flag'])\n",
    "\n",
    "# Encode categorical columns if necessary, basically putting a number tag on text data in the cells\n",
    "label_encoders = {}\n",
    "for column in ['protocol_type', 'service', 'flag']:\n",
    "    le = LabelEncoder()\n",
    "    data[column] = le.fit_transform(data[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Concatinating the removed columns (ones who had text) and replacing them with numbers\n",
    "data_preprocessed = pd.concat([data_numeric.drop(columns=['class']), data[['protocol_type', 'service', 'flag']]], axis=1)\n",
    "\n",
    "# Extract the target variable\n",
    "y = data['class'].map({'normal': 0, 'anomaly': 1})\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data_preprocessed)\n",
    "\n",
    "# Step 4: Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=10)\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train = principal_components[:20000]\n",
    "y_train = y[:20000]\n",
    "X_test = principal_components[20000:]\n",
    "y_test = y[20000:]\n",
    "\n",
    "# Step 5: Train logistic regression model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model accuracy: {accuracy*100:.2f}\")\n",
    "\n",
    "classification_report_test = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report for Test Set:\")\n",
    "print(classification_report_test)\n",
    "\n",
    "# Save the scaler, PCA, and logistic regression model to disk\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(pca, 'pca.pkl')\n",
    "joblib.dump(log_reg, 'log_reg.pkl')\n",
    "joblib.dump(label_encoders, 'label_encoders.pkl')\n",
    "\n",
    "# Print out the first few rows to see the results\n",
    "print(data.head())\n",
    "\n",
    "# Save the preprocessed data as a new CSV file\n",
    "data.to_csv(r\"C:\\Users\\shubh\\OneDrive\\Desktop\\SOC_cyberguard\\Preprocessed_Train_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b79a6e22-1ee5-4c9d-83d9-85b7540dd89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 99.08\n",
      "Classification Report for Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      2756\n",
      "           1       0.99      0.99      0.99      2436\n",
      "\n",
      "    accuracy                           0.99      5192\n",
      "   macro avg       0.99      0.99      0.99      5192\n",
      "weighted avg       0.99      0.99      0.99      5192\n",
      "\n",
      "   duration  protocol_type  service  flag  src_bytes  dst_bytes  land  \\\n",
      "0         0              1       19     9        491          0     0   \n",
      "1         0              2       41     9        146          0     0   \n",
      "2         0              1       46     5          0          0     0   \n",
      "3         0              1       22     9        232       8153     0   \n",
      "4         0              1       22     9        199        420     0   \n",
      "\n",
      "   wrong_fragment  urgent  hot  ...  dst_host_srv_count  \\\n",
      "0               0       0    0  ...                  25   \n",
      "1               0       0    0  ...                   1   \n",
      "2               0       0    0  ...                  26   \n",
      "3               0       0    0  ...                 255   \n",
      "4               0       0    0  ...                 255   \n",
      "\n",
      "   dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
      "0                    0.17                    0.03   \n",
      "1                    0.00                    0.60   \n",
      "2                    0.10                    0.05   \n",
      "3                    1.00                    0.00   \n",
      "4                    1.00                    0.00   \n",
      "\n",
      "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
      "0                         0.17                         0.00   \n",
      "1                         0.88                         0.00   \n",
      "2                         0.00                         0.00   \n",
      "3                         0.03                         0.04   \n",
      "4                         0.00                         0.00   \n",
      "\n",
      "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
      "0                  0.00                      0.00                  0.05   \n",
      "1                  0.00                      0.00                  0.00   \n",
      "2                  1.00                      1.00                  0.00   \n",
      "3                  0.03                      0.01                  0.00   \n",
      "4                  0.00                      0.00                  0.00   \n",
      "\n",
      "   dst_host_srv_rerror_rate    class  \n",
      "0                      0.00   normal  \n",
      "1                      0.00   normal  \n",
      "2                      0.00  anomaly  \n",
      "3                      0.01   normal  \n",
      "4                      0.00   normal  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Step 2: Load the dataset  # Replace with your CSV file path\n",
    "data = pd.read_csv(r\"C:\\Users\\shubh\\OneDrive\\Desktop\\SOC_cyberguard\\Train_data.csv\")\n",
    "\n",
    "# Step 3: Preprocess the data\n",
    "# Drop non-numeric columns except 'class'\n",
    "data_numeric = data.drop(columns=['protocol_type', 'service', 'flag'])\n",
    "\n",
    "# Encode categorical columns if necessary, basically putting a number tag on text data in the cells\n",
    "label_encoders = {}\n",
    "for column in ['protocol_type', 'service', 'flag']:\n",
    "    le = LabelEncoder()\n",
    "    data[column] = le.fit_transform(data[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Concatenate the removed columns (ones who had text) and replace them with numbers\n",
    "data_preprocessed = pd.concat([data_numeric.drop(columns=['class']), data[['protocol_type', 'service', 'flag']]], axis=1)\n",
    "\n",
    "# Extract the target variable\n",
    "y = data['class'].map({'normal': 0, 'anomaly': 1})\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data_preprocessed)\n",
    "\n",
    "# Step 4: Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=10)\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train = principal_components[:20000]\n",
    "y_train = y[:20000]\n",
    "X_test = principal_components[20000:]\n",
    "y_test = y[20000:]\n",
    "\n",
    "# Step 5: Train Decision Tree Classifier model\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model accuracy: {accuracy*100:.2f}\")\n",
    "\n",
    "classification_report_test = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report for Test Set:\")\n",
    "print(classification_report_test)\n",
    "\n",
    "# Save the scaler, PCA, and Decision Tree model to disk\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(pca, 'pca.pkl')\n",
    "joblib.dump(decision_tree, 'decision_tree.pkl')\n",
    "joblib.dump(label_encoders, 'label_encoders.pkl')\n",
    "\n",
    "# Print out the first few rows to see the results\n",
    "print(data.head())\n",
    "\n",
    "# Save the preprocessed data as a new CSV file\n",
    "data.to_csv(r\"C:\\Users\\shubh\\OneDrive\\Desktop\\SOC_cyberguard\\Preprocessed_Train_data_2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4df710e1-1400-494a-9ee9-090364f66a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 99.15\n",
      "Classification Report for Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      2756\n",
      "           1       0.99      0.99      0.99      2436\n",
      "\n",
      "    accuracy                           0.99      5192\n",
      "   macro avg       0.99      0.99      0.99      5192\n",
      "weighted avg       0.99      0.99      0.99      5192\n",
      "\n",
      "\n",
      "   duration  protocol_type  service  flag  src_bytes  dst_bytes  land  \\\n",
      "0         0              1       19     9        491          0     0   \n",
      "1         0              2       41     9        146          0     0   \n",
      "2         0              1       46     5          0          0     0   \n",
      "3         0              1       22     9        232       8153     0   \n",
      "4         0              1       22     9        199        420     0   \n",
      "\n",
      "   wrong_fragment  urgent  hot  ...  dst_host_srv_count  \\\n",
      "0               0       0    0  ...                  25   \n",
      "1               0       0    0  ...                   1   \n",
      "2               0       0    0  ...                  26   \n",
      "3               0       0    0  ...                 255   \n",
      "4               0       0    0  ...                 255   \n",
      "\n",
      "   dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
      "0                    0.17                    0.03   \n",
      "1                    0.00                    0.60   \n",
      "2                    0.10                    0.05   \n",
      "3                    1.00                    0.00   \n",
      "4                    1.00                    0.00   \n",
      "\n",
      "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
      "0                         0.17                         0.00   \n",
      "1                         0.88                         0.00   \n",
      "2                         0.00                         0.00   \n",
      "3                         0.03                         0.04   \n",
      "4                         0.00                         0.00   \n",
      "\n",
      "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
      "0                  0.00                      0.00                  0.05   \n",
      "1                  0.00                      0.00                  0.00   \n",
      "2                  1.00                      1.00                  0.00   \n",
      "3                  0.03                      0.01                  0.00   \n",
      "4                  0.00                      0.00                  0.00   \n",
      "\n",
      "   dst_host_srv_rerror_rate    class  \n",
      "0                      0.00   normal  \n",
      "1                      0.00   normal  \n",
      "2                      0.00  anomaly  \n",
      "3                      0.01   normal  \n",
      "4                      0.00   normal  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Step 2: Load the dataset  # Replace with your CSV file path\n",
    "data = pd.read_csv(r\"C:\\Users\\shubh\\OneDrive\\Desktop\\SOC_cyberguard\\Train_data.csv\")\n",
    "\n",
    "# Step 3: Preprocess the data\n",
    "# Drop non-numeric columns except 'class'\n",
    "data_numeric = data.drop(columns=['protocol_type', 'service', 'flag'])\n",
    "\n",
    "# Encode categorical columns if necessary, basically putting a number tag on text data in the cells\n",
    "label_encoders = {}\n",
    "for column in ['protocol_type', 'service', 'flag']:\n",
    "    le = LabelEncoder()\n",
    "    data[column] = le.fit_transform(data[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Concatenate the removed columns (ones who had text) and replace them with numbers\n",
    "data_preprocessed = pd.concat([data_numeric.drop(columns=['class']), data[['protocol_type', 'service', 'flag']]], axis=1)\n",
    "\n",
    "# Extract the target variable\n",
    "y = data['class'].map({'normal': 0, 'anomaly': 1})\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data_preprocessed)\n",
    "\n",
    "# Step 4: Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=10)\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train = principal_components[:20000]\n",
    "y_train = y[:20000]\n",
    "X_test = principal_components[20000:]\n",
    "y_test = y[20000:]\n",
    "\n",
    "# Step 5: Train K-Nearest Neighbors model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model accuracy: {accuracy*100:.2f}\")\n",
    "\n",
    "classification_report_test = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report for Test Set:\")\n",
    "print(classification_report_test)\n",
    "print()\n",
    "\n",
    "# Save the scaler, PCA, and KNN model to disk\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(pca, 'pca.pkl')\n",
    "joblib.dump(knn, 'knn.pkl')\n",
    "joblib.dump(label_encoders, 'label_encoders.pkl')\n",
    "\n",
    "# Print out the first few rows to see the results\n",
    "print(data.head())\n",
    "\n",
    "# Save the preprocessed data as a new CSV file\n",
    "data.to_csv(r\"C:\\Users\\shubh\\OneDrive\\Desktop\\SOC_cyberguard\\Preprocessed_Train_data_3.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17ba3de-e9c7-4afd-a751-6f41e6dbb353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
